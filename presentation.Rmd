---
title: "Intro to statistics for computational genomics"
author: "Davide Risso"
date: "September 14, 2016"
output:
  beamer_presentation:
    toc: yes
    fig_caption: false
    includes:
      in_header: header.tex
    slide_level: 2
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, error=FALSE, echo=TRUE, results = "markup", fig.align = "center", tidy = FALSE, autodep = TRUE)
```

# Introduction

## Acknowledgements

Many of these slides are based or inspired by the material developed by Sandrine Dudoit for the course "Computational Statistics with Applications in Biology and Medicine" tought at UC Berkeley.

## About me

- I am a postdoc in the Division of Biostatistics, University of California, Berkeley
- My research focuses on the development of statistical methods and software to address problems in biomedical and genomic applications.
- I am author and maintainer of four R/Bioconductor packages

## Why statistics?

\begin{quote}
\emph{I keep saying that the sexy job in the next 10 years will be statisticians, and I’m not kidding.}
\end{quote}

\source{Hal Varian, Chief Economist, Google (2009)}

## Why statistics?

\begin{quote}
\emph{The coming century is surely the century of data}
\end{quote}

\begin{flushright}
\scriptsize David Donoho (2000)
\end{flushright}

\begin{quote}
\emph{Statistical thinking will one day be as necessary for efficient citizenship as the ability to read or write}
\end{quote}

\begin{flushright}
\scriptsize Attributed to H. G. Wells by Darrell Huff (1954)
\end{flushright}

\begin{quote}
\emph{Statistics is the grammar of science}
\end{quote}

\begin{flushright}
\scriptsize Karl Pearson (1892)
\end{flushright}

## Why should \textit{I} learn statistics?

\centering
\includegraphics[width=.95\linewidth]{figures/extrapolating}

\source{http://xkcd.com/605/}

## Why should \textit{I} learn statistics?

\centering
\includegraphics[width=.95\linewidth]{figures/correlation}

\source{http://xkcd.com/552/}

## Why should \textit{I} learn statistics?

\begin{quote}
\emph{Those who ignore Statistics are condemned to reinvent it}
\end{quote}

\begin{flushright}
\scriptsize Attributed to Bradley Efron by Jerome H. Friedman (2001)
\end{flushright}

\begin{quote}
\emph{To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.}
\end{quote}

\begin{flushright}
\scriptsize R. A. Fisher (1938)
\end{flushright}

## Why should \textit{I} learn statistics?

\begin{quote}
\emph{Expression genetic data may not require a new breed of scientist, but they probably will require a new breed of collaboration. The focus of the computational biologist will need to change from the development of tools that answer specific questions to the development of general tools that enable biologists to carry out their own investigations---to explore, visualize and find biological signals in complex data.}
\end{quote}

\source{Karl W. Broman (2005 Nat Genet 37: 209-210)}

## What is statistics?

Many views...

- We muddle through life making _choices based on incomplete information_. (Gonick and Smith, 1993).
- What makes Statistics unique is its _ability to quantify uncertainty_, to make it precise. This allows statisticians to make categorical statements, with complete assurance — about their level of uncertainty. (Gonick and Smith, 1993).
- Statistics is the _art of making numerical conjectures about puzzling questions_. (Freedman et al., 1978).
- The objective of statistics is to make inferences (predictions, decisions) about a population based on information contained in a sample. (Mendenhall, 1987).

## What is statistics?

- \emph{Descriptive statistics:} Summarize the data to highlight *trends* and discover _hidden patterns_.
- \emph{Inference:} make conclusions about a _population_ based on information contained in a _sample_.

## An example dataset: ALL/AML microarray data

Study of gene expression in two types of acute leukemias, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML) [@golub1999molecular].

- Affymetrix high-density oligonucleotide chips (Hu6800 chip): 7,129 different probe-sets.
- 47 cases of ALL (38 B-cell ALL and 9 T-cell ALL) and 25 cases of AML. Training set of 27 ALL and 11 AML.
- R datasets: `Golub_Train`, `Golub_Test`, and `Golub_Merge` in the Bioconductor package `golubEsets`.
- Data already pre-processed (image quantitation, normalization): measured on 16-bit scale, take log base 2.

```{r install, eval=FALSE}
source("https://bioconductor.org/biocLite.R")
biocLite("golubEsets")
```

## An example dataset: ALL/AML microarray data

\small
```{r data_example}
library(golubEsets)

data(Golub_Train)
golub<-exprs(Golub_Train)
golub[golub<100]<-100
golub[golub>16000]<-16000
golub <- log2(golub)
pheno<-pData(Golub_Train)$ALL.AML

head(golub[,1:3])
```

# Numerical and graphical summaries

## Definitions: Data and variables

- The _data_ consist of one or more _variables_ measured/recorded on observational units in a _population_ or _sample_ of interest.
- The term _variable_ refers to characteristics that differ among observational units.
- E.g., expression of genes (variables) in patients (observational units)

## Definitions: Data and variables

_Quantitative/Numerical_ variables

- _Continuous_ (real numbers): any value corresponding to the points in an interval (e.g., height, cholesterol level).
- _Discrete_ (integers): countable number of values (e.g., T-cell counts).

_Qualitative/Categorical_ variables

- _Nominal_: names or labels, no natural order (e.g, sex, eye color).
- _Ordinal_: ordered categories, no natural numerical scale (e.g., tumor grade).

## Numerical and Graphical Summaries of Data

The goal is to _provide numerical and graphical descriptions of data_ to

- summarize the main features of the data;
- uncover unusual features of the data;
- reveal and summarize relationships between one or more variables and/or observational units.

Useful for _exploratory data analysis_ (EDA) (cf. Tukey):

- quality control
- overall impressions
- outlier detection
- validity of the assumptions of candidate models

and for _displaying and reporting the results_ of a statistical analysis.

## Summarizing the data

How can we get a sense of the values of the gene "IGF2" in this dataset?

\tiny
```{r summary}
ZYX <- golub["X95735_at",]
ZYX
```

## Histograms

\footnotesize
```{r histogram}
library(RColorBrewer)
colors <- brewer.pal(8, "Set2")
hist(ZYX, col=colors[1])
```

## Histograms

A histogram consists of a set of blocks or bins, where the area of each block represents the percentage of observations in the corresponding class interval.

\bigskip

The unit of the vertical axis is percent per unit of the horizontal axis.

\bigskip

The choice of the number of bins (argument `breaks`) greatly affects the plot.

## Histograms

_Same data_ with different number of bins.

```{r histogram2, echo=FALSE}
par(mfrow=c(2,3))
x1 <- c(rnorm(5000), rnorm(5000,4,2))
B <- c(5,10,20,50,100)
for(b in B)
 hist(x1, probability=TRUE, breaks=b, main=paste("breaks=",b,sep=""))
hist(x1[1:5000],probability=TRUE,xlim=range(x1),xlab="x1",main="",col=colors[1])
hist(x1[5001:10000],probability=TRUE,col=colors[2],add=TRUE)
par(mfrow=c(1,1))
```

## Density plots

Smoothed versions of histograms, using _kernel density estimators_.

```{r density}
plot(density(ZYX), main="Density plot of ZYX")
```

## Density plots

Smoothed versions of histograms, using _kernel density estimators_.

```{r density2}
hist(ZYX, col="gray", probability=TRUE)
lines(density(ZYX), lwd=3, col=colors[2])
```

## Density plots

- The choice of bandwidth is very important and can have a large impact on the density estimator.
- The larger the bandwidth, the smoother the estimator.
- This is an example of a bias-variance trade-off; as the
bandwidth increases, variance decreases but bias increases.

## Density plots

```{r density3, echo=FALSE}
hist(ZYX, col="gray", probability=TRUE, ylim=c(0, 0.4), main="Density plots: different bandwidths")
lines(density(ZYX, bw=1), lwd=3, col=colors[1])
lines(density(ZYX, bw=2), lwd=3, col=colors[2])
lines(density(ZYX, bw=0.5), lwd=3, col=colors[3])
lines(density(ZYX, bw=0.1), lwd=3, col=colors[4])
legend("topright", as.character(c(0.1, 0.5, 1, 2)), fill=colors)
```

## Boxplots

- The _boxplot_, also called _box-and-whisker plot_, was proposed by Tukey (1977) as a simple graphical summary of the distribution of a variable.
- The summary consists of the median, the upper and lower quartiles, the range, and possibly individual extreme values.

## Boxplots

```{r boxplot_anatomy, echo=FALSE}
set.seed(123)
ab <- boxplot(c(rnorm(100), -2.75, 2.5, 3.0), main = "Anatomy of a boxplot")
arrows(x0 = .75, y0 = ab$out, x1 = .95, y1 = ab$out)
text(x = .65, y = ab$out, labels = rep('outlier', length(ab$out)))
arrows(x0 = .6, ab$stats, x1 = .75, ab$stats,col=c("magenta","cyan","cyan","cyan","magenta"))
text(x = .55, y = ab$stats[1,], labels = "A",col="magenta")
text(x = .55, y = ab$stats[5,], labels = "B",col="magenta")
text(x = .55, y = ab$stats[2,], labels = expression(q[.25]),col="cyan")
text(x = .55, y = ab$stats[3,], labels = expression(q[.5]),col="cyan")
text(x = .55, y = ab$stats[4,], labels = expression(q[.75]),col="cyan")
```

\source{J. H. Bullard, K. D. Hansen, and M. Taub (Summer 2008). Statistics with R for Biologists: A Short Course.}

## Boxplot

- The line in the middle of the box represents the median, a robust measure of the center or location of the distribution.
- The upper and lower sides of the box are the upper and lower quartiles, respectively.
- The central box represents the inter-quartile range (IQR), a robust measure of the spread or scale of the distribution.
- Extreme values, more than 1.5 IQR above the upper quartile and below the lower quartile, are typically plotted individually.

## Boxplot

```{r boxplot_zyx}
boxplot(ZYX, xlab="ZYX", ylab="Expression measure",
        col=colors[1], main="Boxplot for Zyxin")
```

## Boxplot

```{r boxplot_pheno}
boxplot(ZYX~pheno, xlab="ZYX", ylab="Expression measure",
        col=colors[1:2], main="Boxplot for Zyxin")
```

## Numerical summaries

What if we want to _quantify_ the differences between the distributions?

As an example, consider the following four sets of 10,000 numbers.

```{r numerical, echo=FALSE}
x3 <- data.frame(cbind(rnorm(10000,0,1),rnorm(10000,5,1),rnorm(10000,0,2),rchisq(10000,2)-2))
plot(density(x3[,1]),col=colors[1],lwd=3,xlim=c(-8,17),xlab="",main="Kernel density estimates for four distributions",sub="")
for(j in (2:4))
  lines(density(x3[,j]),col=colors[j+1],lwd=3)
```

## Location, spread, range

Consider a list of $n$ real numbers, $\{x_1, ..., x_n\}$.

The _mean_  measures the _center_ or _location_ of a distribution,
$$
\bar{x} \equiv \frac{\mbox{sum of values}}{\mbox{number of values}} = \frac{\sum_{i=1}^n x_i}{n}.
$$

\bigskip

The _standard deviation_ (SD) measures the _spread_ or _scale_ of a distribution.
\begin{eqnarray*}
s_x &\equiv& \sqrt{\mbox{mean of (deviations from the mean)$^2$}}\\
&=& \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n}}.
\end{eqnarray*}

\bigskip

The _variance_ is the square of the standard deviation.

\bigskip

For the Gaussian distribution, roughly 
68\% of the observations are within one SD of the mean and 
95\% within two SD.

R functions: `mean`, `sd`, `var`.

## Robust alternatives

The _median_ is another measure describing the center of a distribution.

It is more _robust_ than the mean, in the sense that it is less affected by extreme values, i.e., by observations in the tails of the distribution.

\bigskip

Similarly, robust measures of _spread_ are the _inter-quartile range_ (IQR),
$$ IQR \equiv \mbox{upper quartile} - \mbox{lower quartile},$$
and the _median absolute deviation_ (MAD),
$$ MAD \equiv \mbox{median of $|$deviations from median$|$}.$$

\bigskip

R functions: `median`, `mad`, and `IQR`.

## Robustness

\small
```{r robustness}
x <- c(rnorm(100, mean=0, sd=1), 1000)
mean(x)
median(x)
sd(x)
mad(x)
IQR(x)
```

## Relation between two variables: correlation

The _correlation coefficient_ is a measure of _linear association_. It is defined as the mean of the product of the observations in standard units,
$$
r= \text{Cor}[x,y]  \equiv \frac{1}{n} \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{s_x}\right) \left(\frac{y_i - \bar{y}}{s_y} \right).
$$

\bigskip

Correlation coefficients are always _between -1 and 1_. 

A positive (negative) correlation means that the cloud of points in the scatterplot slopes up (down). 

\bigskip

The correlation coefficient is _without units_. 

It is not affected by: adding a constant to all the values of one variable; multiplying all the values of one variable by a positive constant.

\bigskip

_Spearman's rank correlation coefficient_ is a robust measure of correlation, where ranks are used in place of the actual values of the $x$ and $y$ variables.

## Scatterplot

\scriptsize

```{r scatterplot}
golub2 <- t(golub[c("X95735_at", "U41813_at"),])
plot(golub2, main="Scatterplot of the expression of 2 genes",
     pch=19, cex=1.5)
```

## Smoothing

When there are many points, scatterplots are not very informative because of _overplotting_.

```{r scatter2}
golub_samples <- golub[, 1:2]
plot(golub_samples, xlab="Sample 1", ylab="Sample2",
     main="Scatterplot of the gene expression of 2 samples")
```

## Smoothing

One solution is _smoothing_.

```{r scatter3}
smoothScatter(golub_samples, xlab="Sample 1", ylab="Sample2",
     main="Scatterplot of the gene expression of 2 samples")
```

## Smoothing

Adding a trend with _lowess regression_ can reveal hidden patterns.

```{r scatter4}
smoothScatter(golub_samples, xlab="Sample 1", ylab="Sample2",
     main="Scatterplot of the gene expression of 2 samples")
lines(lowess(golub_samples), lwd=3, col=2)
```

## The importance of EDA

Different numerical and graphical summaries highlight different aspects of the data.

```{r correlation, echo=FALSE}
library(MASS)
par(mfrow=c(2, 2))
x1 <- mvrnorm(100,c(0,0),matrix(c(1,0,0,1),2,2,byrow=TRUE))
boxplot(data.frame(x1),
main=paste("Cor[x,y]  = ",round(cor(x1[,1],x1[,2]),2), sep="")
)
x2 <- mvrnorm(100,c(0,0),matrix(c(1,0.99,0.99,1),2,2,byrow=TRUE))
boxplot(data.frame(x2),
main=paste("Cor[x,y]  = ",round(cor(x2[,1],x2[,2]),2), sep="")
)
plot(x1,xlab="x",ylab="y",
main=paste("Cor[x,y]  = ",round(cor(x1[,1],x1[,2]),2), sep="")
)
plot(x2,xlab="x",ylab="y",
main=paste("Cor[x,y]  = ",round(cor(x2[,1],x2[,2]),2), sep="")
)
par(mfrow=c(1, 1))
```

## The importance of EDA

```{r correlation2, echo=FALSE}
x <- rnorm(500)
y <- -x^2 + rnorm(500,0.,0.5)
plot(x,y,main=paste("Cor[x,y] = ", round(cor(x,y),2), sep=""))
abline(lm(y~x)[[1]],col=2,lwd=3)
lines(lowess(y ~ x,f=0.2),col=3,lwd=3)
legend("bottom",c("Ordinary linear regression","Lowess regression"),col=2:3,lwd=3)
```

## Exercise: Displaying high-dimensional data

_What if we have more than two variables?_

\bigskip

Given the `golub` dataset:

1. Find the top 5 most variable genes (hint: function `rowVars` in the package `matrixStats`).
2. Visualize their expression levels (hint: functions `pairs`, `heatmap.2`)

## Solution: Displaying high-dimensional data

\footnotesize
```{r top5}
library(matrixStats)
vars <- rowVars(golub)
names(vars) <- rownames(golub)
vars <- sort(vars, decreasing = TRUE)
golub5 <- golub[names(vars)[1:5],]
```

```{r visualize, eval=FALSE}
heatmap(golub5)
NMF::aheatmap(golub5, color = clusterExperiment::seqPal5,
              annCol = data.frame(Class=pheno))
pairs(t(golub5), pch=20, col=colors[pheno])
boxplot(t(golub5), las=2, ylab="Expression levels")

cors <- cor(t(golub5))
heatmap(cors, col=clusterExperiment::seqPal3)
```

# Dimensionality reduction

## Dimensionality reduction

_Data_ consist of _variables recorded on observational units_.

\bigskip

The data for $J$ variables and $n$ observations can be represented as an $n \times J$ _data matrix_ $X = (X_{i,j}: i=1,\dotsc,n; j=1,\dotsc,J)$. 

\bigskip

In genomic settings, the number of variables $J$ is often in the tens of thousands and the sample size $n \ll J$.

## Dimensionality reduction

_Dimensionality reduction_, i.e., representing the data using _fewer than $J$ variables_, is useful for _summarizing_ and _visualizing_ data, in the context of exploratory data analysis (EDA, e.g., detecting main features), quality assessment/control (QA/QC, e.g., detecting artifacts, outliers), and reporting of results (e.g., clusters).

\bigskip
A variety of often related approaches can be used, but we focus on PCA.

\bigskip
_Principal component analysis_ (PCA) replaces the original variables by fewer _orthogonal linear combinations_ of these variables, with successively _maximal variance_ [@Mardiaetal79] (Chapter 8).

## Principal Component Analysis

_Principal component analysis_ (PCA) is a dimensionality reduction technique that provides a parsimonious summarization of the data by replacing the original variables by fewer _linear combinations_ of these variables, that are _orthogonal_ and have successively _maximal variance_.  

\bigskip
Such linear combinations seek to "separate out" the observations, while loosing as little information as possible.

## Principal Component Analysis

```{r pca_ex, echo=FALSE}
x <- rnorm(100)
y <- .5 * x + rnorm(100, sd=.5)
xy <- cbind(x, y)

xynorm <- scale(xy, scale = FALSE, center = TRUE)
e <- eigen(cov(xynorm))

plot(xynorm)
lines(xynorm[,1], e$vectors[2,1]/e$vectors[1,1] * xynorm[,1], col=2, lwd=3)
lines(xynorm[,1], e$vectors[2,2]/e$vectors[1,2] * xynorm[,1], col=4, lwd=3)
legend("topleft", c("PC1", "PC2"), lty=1, col=c(2, 4), lwd=3)
```

## Principal Component Analysis

```{r pca_ex2, echo=FALSE}
pca <- prcomp(xynorm, center=FALSE)
plot(pca$x)
```

## More formally

Consider a $J$-dimensional random vector $X=(X_j: j=1,\dotsc,J) \in \RR^J$, with distribution $P$, mean vector $\mu = \EV[X]$, and covariance matrix $\Sigma = \Cov[X] = \EV[(X-\mu)(X-\mu)^{\top}]$. 

\bigskip
The _principal component transformation_ of the random vector $X$ is defined as the standardized linear combination 
\begin{equation}
X \rightarrow Y = \Gamma^{\top}(X - \mu),
\end{equation}
where, from the Spectral Decomposition Theorem, $\Gamma^{\top} \Sigma \Gamma = \Lambda$, 
$\Lambda = \text{Diag}(\lambda_j: j=1,\dotsc,J)$ is the diagonal matrix of eigenvalues of $\Sigma$, ordered as $\lambda_1 \geq \lambda_2 \geq \dotsc \geq \lambda_J \geq 0$, and $\Gamma = (\gamma_j: j=1,\dotsc,J)$ is an orthogonal matrix whose columns $\gamma_j$ are the corresponding eigenvectors. 

## More formally

The _$j$th principal component_ ($j$th PC or $PC_j$) of $X$ is then
\begin{equation}
Y_j = \gamma_j^{\top}(X-\mu),
\end{equation}
where the $j$th eigenvector $\gamma_j$ is referred to as the _$j$th vector of principal component loadings_. 

## PCA properties

The principal components satisfy the following properties.

1.
$$
\EV[Y] = 0_J,
$$
where $0_J$ is a $J$-dimensional column vector of zeroes.
2.
$$
\Cov[Y] = \Lambda = \text{Diag}(\lambda_j: j=1,\dotsc,J)
$$
and, in particular,
$$
\text{Var}[Y_1] \geq \text{Var}[Y_2] \geq \dotsc \geq \text{Var}[Y_J].
$$
3.
$$
\sum_{j=1}^J \text{Var}[Y_j] = \tr \Sigma = \sum_{j=1}^J \lambda_j.
$$
4.
$$
\prod_{j=1}^J \text{Var}[Y_j] = |\Sigma| = \prod_{j=1}^J \lambda_j.
$$

## PCA properties

The principal components are _not scale-invariant_!
Depending on the types of variables under consideration, scaling or use of the correlation matrix instead of the covariance matrix may be in order.

\bigskip
The _proportion of total variation explained_ by the first $K$ principal components is given by
\begin{equation}
p_K = \frac{\sum_{j=1}^K \lambda_j}{\sum_{j=1}^J \lambda_j}.
\end{equation}

\bigskip
The _scree plot_, i.e., a plot of $\lambda_j$ vs. $j$, can be used to determine how many components to use (stop at "elbow").

## R software

- `princomp` and `prcomp` can be used to compute the principal components.
- `svd` can be used to obtain the singular value decomposition.
- `eigen` to obtain the eigenvectors and eigenvalues.

## Example

```{r pca_golub}
pca <- prcomp(t(golub))
plot(pca$x, pch=19, col=colors[pheno])
legend("topleft", levels(pheno), fill=colors)
```

## Example

```{r pca_golub2, echo=FALSE}
plot(pca$sdev^2/sum(pca$sdev^2)*100, xlab="Number of PCs", ylab="Percent of variance explained", type='b', main="Percent of variance explained", lwd=3)
```

# Classification

## Class prediction

Consider a data structure $(X,Y) \sim P$, where $Y \in
  \{1,\dotsc,K\}$ is a _scalar polychotomous outcome_ (a.k.a.,
  dependent variable, response) and $X=(X_j: j=1, \dotsc, J) \in
  \RR^J$ is a $J$-dimensional vector of _covariates_ (a.k.a.,
  explanatory, feature, independent, or predictor variables).

\bigskip
The task is to _classify_ an observation, i.e.,
_predict class_ $Y$, _given covariates_ $X$.

\bigskip
E.g. Predict tumor class or response to treatment $Y$, given
  thousands of microarray or high-throughput sequencing measures of
  gene expression $X$.

\bigskip
A _classification function_ or, in short, a _classifier_, is a
  mapping, $\psi: {\cal X} \rightarrow \{1,\dotsc,K\}$, from a
  $J$-dimensional covariate space ${\cal X} \subseteq \RR^J$ into the
  integers $\{1, \dotsc, K\}$.

## Toy example

\framesubtitle{Training set}

```{r toy, echo=FALSE}
library(MASS)

n <- 20
x <- matrix(NA, n, 2)
y <- sample(1:2, n, replace=TRUE)
x[y==1,] <- mvrnorm(sum(y==1),c(1,3),0.5*diag(2))
x[y==2,] <- mvrnorm(sum(y==2),c(-3,-2),0.5*diag(2))

plot(x, pch=19, col=colors[y], xlab=expression(X[1]), ylab=expression(X[2]), cex=2, main="Training set")
```

## Toy example

\framesubtitle{Classification rule}

```{r toy2, echo=FALSE}
plot(x, pch=19, col=colors[y], xlab=expression(X[1]), ylab=expression(X[2]), cex=2, main="Classification rule")
abline(0, -1, lwd=3, lty=2)
```

## Toy example

\framesubtitle{New observation}

```{r toy3, echo=FALSE}
plot(x, pch=19, col=colors[y], xlab=expression(X[1]), ylab=expression(X[2]), cex=2, main="New observation")
abline(0, -1, lwd=3, lty=2)
points(0, 2, pch=1, cex=2.2, lwd=3)
```

## Toy example

\framesubtitle{Prediction}

```{r toy4, echo=FALSE}
plot(x, pch=19, col=colors[y], xlab=expression(X[1]), ylab=expression(X[2]), cex=2, main="Prediction")
abline(0, -1, lwd=3, lty=2)
points(0, 2, pch=1, cex=2.5, lwd=3)
points(0, 2, pch=19, cex=2, col=colors[1])
```

## Class prediction

A classifier generates a _partition of the covariate space_
  ${\cal X}$ into $K$ disjoint and exhaustive subsets, ${\cal C}_1,
  \dotsc, {\cal C}_K$, such that for an observation with covariates $X
  \in {\cal C}_k$ the predicted class is $k$. 
  That is,
\begin{equation}
\psi(X) = \sum_{k=1}^K k\, \Indic(X \in {\cal C}_k).
\end{equation}

\bigskip
As regression, classification can be handled within the
  framework of _loss-based inference_. That is, classifiers
 (parameters and estimators thereof) can be defined in terms of a
  _loss function_ $L((X,Y),\psi)$ and its associated _risk
    function_
\begin{equation}
\Theta_L(\psi,P) \equiv \int L((x,y),\psi) dP(x,y).
\end{equation} 

## Loss function

A widely-used loss function is the _indicator_,
\begin{eqnarray}
  L((X,Y),\psi) &=& \Indic(Y \neq \psi(X))  \\
  &=& \begin{cases}
    1, & \text{if $Y \neq \psi(X)$ [incorrect classification]}\\
    0, & \text{if $Y = \psi(X)$ [correct classification]}
\end{cases}. \nonumber
\end{eqnarray}
The corresponding loss matrix $\mathbf{L}$ has diagonal elements equal to zero and off-diagonal elements equal to one. 
The _risk_ function is the _misclassification probability_
\begin{equation}
\Theta_L(\psi,P) =  \EV[\Indic(Y \neq \psi(X))] = \Pr(Y \neq \psi(X)).
\end{equation}

\bigskip
Note, that $L$ does not need to be symmetric, e.g., it can be more harmful to have false negatives than false positives (or vice versa).

## Example of classifier partitions

Simulate a learning set ${\cal L}_n = \{(X_i,Y_i): i=1,\dotsc,n
  \}$ of $n=200$ independent and identically distributed (IID) $(X,Y)$
  pairs, such that $Y \sim \distn{U}(\{1,\dotsc, K\})$ and $X|Y=k \sim
  \distn{N}(\mu_k,\Sigma_k)$, $k=1,\dotsc,K$, with $K=3$ classes and
  $J=2$ covariates.

\bigskip
The following figures compare the _partitions_ produced by different classifiers based on the learning set ${\cal L}_n$: _linear discriminant analysis_ (LDA), _quadratic discriminant analysis_ (QDA), _na\"ive Bayes_, _$k$-nearest-neighbor_ ($k$-NN), with $k=1,3,5$.

## Example of classifier partitions

```{r classifiers, include=FALSE}
library(MASS)
library(e1071)
library(class)

# Learning set 
n <- 200
x <- matrix(NA, n, 2)
y <- sample(1:3, n, replace=TRUE)
x[y==1,] <- mvrnorm(sum(y==1),c(1,0),0.5*diag(2))
x[y==2,] <- mvrnorm(sum(y==2),c(0,2),diag(2))
x[y==3,] <- mvrnorm(sum(y==3),c(0,0),rbind(c(0.5,0.25),c(0.25,0.5)))
ls <- 1:nrow(x)

# Partitions  
N <-  200
xx <- seq(-4, 4, length=N)
y.lda <- y.qda <- y.nb <- y.knn1 <- y.knn3 <- y.knn5 <- y.knn9 <- y.rpart <- matrix(NA, N, N)

for(i in 1:N)
{
  x.new <- data.frame(X1=xx[i],X2=xx)
  dat <- data.frame(x,Y=factor(y))
  dat <- rbind(dat,cbind(x.new,Y=rep(NA,length(xx))))

  fit.lda <- lda(Y ~ . , data=dat,subset=ls)
  y.lda[i,] <- predict(fit.lda,dat[-ls,])$class
  
  fit.qda <- qda(Y ~ . , data=dat,subset=ls)
  y.qda[i,] <- predict(fit.qda,dat[-ls,])$class
    
  fit.nb <-naiveBayes(Y ~ ., data=dat, subset=ls,type="class")
  y.nb[i,] <- predict(fit.nb,dat[-ls,])
    
  y.knn1[i,] <- knn(x,x.new,y,k=1)
  y.knn3[i,] <- knn(x,x.new,y,k=3)
  y.knn5[i,] <- knn(x,x.new,y,k=5)

}

#########################
myPlotPartition <- function(x,y,xx,y.hat,col1=1,col2=heat.colors(12),pch=as.character(y))
{
  image(xx,xx,y.hat,col=col2,xlab=expression(X[1]),ylab=expression(X[2]))
  points(x,pch=pch,col=col1[y])
}

```

```{r sim, echo=FALSE}
plot(x,xlab=expression(X[1]),ylab=expression(X[2]),col=colors[y], pch=19, main="Simulated data")
legend("topright",c("Y=1", "Y=2", "Y=3"),fill=colors)
```

## Example of classifier partitions: LDA

```{r lda, echo=FALSE}
col1 <- c(rgb(0.5,0,0),rgb(0,0.5,0),rgb(0,0,0.5))
col2 <- colors
pch <- 16

myPlotPartition(x,y,xx,y.lda,col1=col1,col2=col2,pch=pch)
```

## Example of classifier partitions: QDA

```{r qda, echo=FALSE}
myPlotPartition(x,y,xx,y.qda,col1=col1,col2=col2,pch=pch)
```

## Example of classifier partitions: NB

```{r nb, echo=FALSE}
myPlotPartition(x,y,xx,y.nb,col1=col1,col2=col2,pch=pch)
```

## Example of classifier partitions: k-NN (k=1)

```{r knn1, echo=FALSE}
myPlotPartition(x,y,xx,y.knn1,col1=col1,col2=col2,pch=pch)
```

## Example of classifier partitions: k-NN (k=3)

```{r knn3, echo=FALSE}
myPlotPartition(x,y,xx,y.knn3,col1=col1,col2=col2,pch=pch)
```

## Example of classifier partitions: k-NN (k=5)

```{r knn5, echo=FALSE}
myPlotPartition(x,y,xx,y.knn5,col1=col1,col2=col2,pch=pch)
```

## Class prediction vs. cluster analysis

- _Classification_. _Assign observational units to classes_ on the basis of
variables characterizing/describing these observations.
- _Cluster analysis._ The _classes are undefined_ a priori and need to be ``discovered'' from the data.
    - a.k.a. Class discovery, unsupervised learning, unsupervised pattern recognition.
- _Class prediction._ The _classes are predefined_ and the task is to understand the basis for the classification from a set of labeled observations, i.e., a learning set. This information is then used to _predict the class of future observations_.
    - a.k.a. Classification, discriminant analysis, supervised learning, supervised pattern recognition.

## General issues in class prediction

- _Variable selection_. Filtering out uninteresting features.
- _Standardization of observations and/or variables_ (cf.
normalization).
- _Distance measure._
- _Loss function._
- _Class representation._
- _Imputation of missing data._
- _Polychotomous classification._ For classifiers that can only handle binary outcomes.
- _Confidence in prediction._

## Performance assessment

The _true risk_ is typically unknown, as it depends on the unknown population distribution.

\bigskip
The _empirical risk_, i.e., the risk estimated from the learning set is _biased_ and leads to _overfitting_.

\bigskip
The classifier should be _trained_ on the training set and tested on a different _test set_ of observations.

\bigskip
In the absence of a genuine test set, one can use _cross-validation_.

## Performance assessment

In order to provide accurate estimators of risk, it is essential to account for _all aspects_ of the classifier learning process, e.g., variable selection, selection of number of neighbors k and distance function for k-nearest-neighbor (k-NN) classifiers.

## Training vs test error

```{r sim2, echo=FALSE}
# Learning set 
n <- 200
x <- matrix(NA, n, 2)
y <- sample(1:2, n, replace=TRUE)
x[y==1,] <- mvrnorm(sum(y==1),c(1,0),0.5*diag(2))
x[y==2,] <- mvrnorm(sum(y==2),c(0,2),diag(2))

# Test set
n <- 100
xt <- matrix(NA, n, 2)
yt <- sample(1:2, n, replace=TRUE)
xt[yt==1,] <- mvrnorm(sum(yt==1),c(1,0),0.5*diag(2))
xt[yt==2,] <- mvrnorm(sum(yt==2),c(0,2),diag(2))

er1 <- sapply(1:10, function(i) {
  pred1 <- knn(x, x, y, k=i)
  sum(pred1 != y)/length(y)
})

er2 <- sapply(1:10, function(i) {
  pred1 <- knn(x, xt, y, k=i)
  sum(pred1 != yt)/length(yt)
})

ercv <- sapply(1:10, function(i) {
  pred1 <- knn.cv(x, y, k=i)
  sum(pred1 != y)/length(y)
})

plot(1:10, er1, type='b', ylim=c(0, .2), xlab="k", ylab="Misclassification rate", main="k-NN classifier performance", lwd=3)
lines(1:10, er2, type='b', col=2, lwd=3)
legend("topright", c("Training set error", "Test set error"), fill=1:2)
```

## Bayes rule and classification

We are interested in the posterior probability
$$
P(Y=k|X) = \frac{P(Y=k) \, P(X|Y=k)}{\sum_{i=1}^K P(Y=i) \, P(X|Y=i)}
$$

\bigskip
The _classification rule_ is to assign assign the observation to the population with the greatest posterior probability.

\bigskip
If we assume that the joint distribution of X is a multivariate normal, this gives rise to _Linear Discriminant Analysis_ (LDA) and _Quadratic Discriminant Analysis_ (QDA).

\bigskip
LDA assumes homogeneous variance across the populations, while QDA is the more general case of heterogeneous variances.

## Quadratic Discriminant Analysis

Let us assume
$$
P(X|Y=k) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp{-\frac{1}{2} (X-\mu_k)^T \Sigma_k^{-1} (X-\mu_k)}.
$$

\bigskip
Hence we need to minimize
$$
\psi(X) = \argmin_k \ \left\{(X - \mu_k)^{\top} \Sigma_k^{-1} (X - \mu_k) + \log |\Sigma_k| - 2 \log p(k) \right\}.
$$

\bigskip
The main quantity in the discriminant rule is $(X- \mu_k)^{\top} \Sigma_k^{-1} (X - \mu_k)$, the _squared Mahalanobis distance_ from the covariate vector $X$ to the class $k$ mean vector $\mu_k$.

\bigskip
Thus, intuitively, the predicted class for an observation with covariates $X$ is the  _class with closest mean vector_ $\mu_k$, for a suitably-defined distance function.


## Linear Discriminant Analysis

If we assume that $\Sigma_k = \Sigma$ for all $k$, we have LDA.

The discriminant rule is based on the square of the Mahalanobis distance and is _linear in the covariates_ $X$,
\begin{eqnarray}
\psi(X) &=& \argmin_k\ (X - \mu_k)^{\top} \Sigma^{-1} (X - \mu_k)\\
&=& \argmin_k\ -2\mu_k^{\top}\Sigma^{-1}X + \mu_k^{\top}\Sigma^{-1}\mu_k. \nonumber
\end{eqnarray}

## Special cases

- _Diagonal Linear Discriminant Analysis_ (DLDA). If we assume a diagonal covariance matrix, $\Sigma_k = \Delta = \Diag(\sigma_1^2,\dotsc,\sigma_J^2)$,  a.k.a. _naive Bayes rule_ for Gaussian class conditional densities.
- _Nearest Centroids_. If we assume that $\Sigma_k = I_J$, the $J \times J$ identity matrix. Observations are classified on the basis of their _Euclidean distances from class means_ $\mu_k$.

## Advantages of LDA and QDA

LDA and QDA are

- _Simple and intuitive_: The predicted class of a new observation is the class with the closest mean (using the Mahalanobis metric).
- _Optimal when the model is true_: LDA is based on the estimated Bayes rule for Gaussian class conditional densities and uniform priors.
- _Easy to implement_: LDA has linear boundaries.
- _Good performance in practice_: In spite of a possibly high bias, the low variance of the LDA estimators of class posterior probabilities often results in low classification error.

## Limitations

- Linear or even quadratic discriminant boundaries may not be flexible enough.
- In the case of a high-dimensional covariate space, performance may degrade rapidly due to over-parameterization and high variance of estimators.

## Extensions

- _Flexible discriminant analysis_ (FDA). LDA on transformed outcomes and/or covariates.
_ _Penalized discriminant analysis_ (PDA). A penalized Mahalanobis distance is used to enforce smoothness.
- _Mixture discriminant analysis_ (MDA). Class conditional densities are modeled as mixtures of Gaussian densities.
- _Automatic variable selection_. Shrinkage methods can be applied to perform automatic variable selection.

See [@Ripley96] and [@HTF01].

## R software

- `lda`, `predict.lda` (`MASS`): Linear discriminant analysis.
- `qda`, `predict.qda` (`MASS`): Quadratic discriminant analysis.
- `naiveBayes` (`e1071`): Diagonal linear discriminant analysis, naive Bayes.

Many arguments and values; consult help files for details and examples.

## Exercise: A classifier for AML vs ALL

\footnotesize

```{r lda_ex}
library(limma)

# Train data
data(Golub_Train)
golub<-exprs(Golub_Train)
golub[golub<100]<-100
golub[golub>16000]<-16000
golub <- log2(golub)
golub <- normalizeQuantiles(golub)
pheno<-pData(Golub_Train)$ALL.AML

# Test data
data(Golub_Test)
golubt<-exprs(Golub_Test)
golubt[golubt<100]<-100
golubt[golubt>16000]<-16000
golubt <- log2(golubt)
golubt <- normalizeQuantiles(golubt)
phenot <- pData(Golub_Test)$ALL.AML
```

## Exercise: A classifier for AML vs ALL

1. Find/Retrieve the most variable genes.
2. Use the top 2, 4, ..., 20 genes to classify AML vs ALL samples with LDA.
3. Evaluate the performance both in the training and in the test set.
4. Plot the training and test set error as a function of the number of genes.

## Solution: A classifier for AML vs ALL

\footnotesize

```{r lda_sol}
vars <- rowVars(golub)
names(vars) <- rownames(golub)
vars <- sort(vars, decreasing = TRUE)

ngenes <- seq(2, 20, by=2)
er1 <- er2 <- numeric(length(ngenes))
for(i in seq_along(ngenes)) {
dat <- data.frame(pheno, t(golub[names(vars)[seq_len(ngenes[i])],]))
newdat <- data.frame(phenot, t(golubt[names(vars)[seq_len(ngenes[i])],]))
fit <- lda(pheno~., data=dat)
pred_ls <- predict(fit, dat)
pred_ts <- predict(fit, newdat)

er1[i] <- sum(pred_ls$class != pheno)/length(pheno)
er2[i] <- sum(pred_ts$class != phenot)/length(phenot)
}
```

## Solution: A classifier for AML vs ALL

\footnotesize
```{r lda_plot}
plot(ngenes, er1, type='b', ylab="Misclassification rate", 
     main="LDA classifier performance", ylim=c(0, max(er2)),
     xlab="Number of genes", lwd=3)
lines(ngenes, er2, type='b', col=2, lwd=3)
legend("topright", c("Training set error", "Test set error"), fill=1:2)
```

## Other classification algorithms

One of the most active area of research in Statistics and Machine Learning.

Some of the most used classifiers are:

- Nearest-neighbor classifiers [@FixHodges51]
- Support Vector Machines (SVM) [@Vapnik98]
- Classification And Regression Trees (CART) [@Breimanetal84]
- Neural Networks (cf. Deep Learning) [@Ripley96]

\bigskip
_Wikipedia has 79 pages in the category "Classification algorithms"_

## Ensemble methods

Substantial gains in accuracy can be obtained from _ensemble methods_, i.e., by _aggregating predictors_. 

\bigskip
For polychotomous outcomes, multiple predictors are aggregated by voting, for continuous outcomes, by averaging [@Dudoit03]. 

\bigskip
One can consider the _same predictor_ built _on perturbed versions of the learning set_, e.g., bootstrap samples, or a _linear combination of/vote from different predictors_.

## Ensemble methods

Examples include

- _Bagging_: Aggregate same predictor built on multiple bootstrap samples of the learning set [@BreimanML96].
- _Boosting_: Aggregate same predictor built from repeated adaptive resampling of the learning set, where sampling weights are increased for observations most often misclassified [@Friedmanetal00].
- _Random forests_: Aggregate trees built on multiple bootstrap samples of the learning set, where covariates are randomly selected for consideration at each node [@BreimanRF01].
- _Super Learner_: Linear combination of different predictors, where coefficients in the combination are selected by cross-validation [@vdLaanetal07].

# Cross-validation

## Cross-validation (CV)

To assess the performance, one cannot use the same observations for both training and testing the classifier.

\bigskip
If one does not have a _genuine test or validation set_, she can  _partition the available learning set_ ${\cal X}_n = \{X_i: i=1,\dotsc,n\}$ into two sets: a _training set_ and a _validation_ set. 

\bigskip
Observations in the training set ${\cal T}_n$ are used to compute, or _train_, an estimator(s).

\bigskip
Observations in the validation set ${\cal V}_n$ are used to assess the risk of, or _validate_, this estimator(s). 

\bigskip
Note that CV can be applied in general estimation problems and is not limited to classification.

## Leave-one-out cross-validation (LOOCV)

In _leave-one-out cross-validation_ (LOOCV), each observation in the learning set is used in turn as the validation set and the remaining $(n-1)$ observations are used as the training set. 

\bigskip
The corresponding distribution of the split vector $V_n$ places probability $1/n$ on each of the $n$ binary vectors $v_n^i = (v_n^i(i^{\prime}) = \Indic(i^{\prime}=i) : i^{\prime}=1, \dotsc, n)$, $i=1,\dotsc,n$. 

\bigskip
The proportion of observations in the validation sets is constant, $\Upsilon_n=1/n$.

## V-fold cross-validation

Given a user-supplied fixed number of folds $V$, $V$-fold cross-validation randomly partitions the learning set into $V$ mutually exclusive and exhaustive sets of approximately equal size. 

\bigskip
Each set is used in turn as the validation set. 

\bigskip
Such a partition results in $V$ binary vectors, $v_n^v = (v_n^v(i): i=1, \dotsc, n)$,  $v=1,\dotsc, V$, such that $\sum_i v_n^v(i) \approxeq n/V$ $\forall \ v$ and $\sum_v v_n^v(i) = 1$ $\forall \ i$. 

\bigskip
The corresponding distribution of the split vector $V_n$ places probability $1/V$ on each of the $V$ binary vectors $v_n^v$.

\bigskip
The proportion of observations in the $V$ validation sets is $\Upsilon_n \approxeq 1/V$.

## V-fold cross-validation

\centering

\includegraphics[angle=0,width=2.5in]{figures/CV1}

Training and validation sets in five-fold cross-validation.

## V-fold cross-validation

\centering

\includegraphics[angle=0,width=2.5in]{figures/CV2}

Training and validation sets in five-fold cross-validation.

## Example: Polynomial regression

```{r regression, echo=FALSE}
set.seed(99)
N <- 100
w <- sort(runif(N, -1.5, 1.5))
y0 <- w*(w-1) * (w+1)  
y <- y0 + rnorm(N, 0, 0.5)

plot(y~w, main="True simulated data, with noise")
lines(y0~w, col=2, lwd=3)
```

## Example: Polynomial regression

How to select the degree of the regression function?

```{r regression2, echo=FALSE}
fit1 <- lm(y~w)
fit2 <- lm(y~w+I(w^2))
fit3 <- lm(y~w+I(w^2)+I(w^3))
fit4 <- lm(y~w+I(w^2)+I(w^3)+I(w^4))

plot(y~w, main="True simulated data, with noise")
lines(w, predict(fit1), col=colors[1], lwd=3)
lines(w, predict(fit2), col=colors[2], lwd=3)
lines(w, predict(fit3), col=colors[3], lwd=3)
lines(w, predict(fit4), col=colors[4], lwd=3)
legend("topleft", paste0("degree ", 1:4), fill=colors)
```

## Example: Polynomial regression

We want to minimize the _mean squared error_ between the prediction and the true values.

```{r risk}
risk <- function(pred, test) {
  mean((pred - test$Y)^2, na.rm=TRUE)
}
```

## Example: Polynomial regression

\footnotesize

```{r pred_lm}
predLM <- function(degree, train, test) {
  f <- c("Y ~ 1")
  for(dd in 1:degree) f <- paste(f,paste("I(W^",dd,")",sep=""),sep="+")
  fit <- lm(f, data=train)
  pred <- predict(fit, test)
  return(pred)
}

D <- 1:20
train <- data.frame(Y=y, W=w)

res <- lapply(D, function(d) predLM(d, train, train))
```

## Example: Polynomial regression

```{r pred_lm_plot, echo=FALSE}
plot(y~w, main="True simulated data, with noise")
lines(y0~w, col=2, lwd=3)
dd <- c(1, 3, 5, 10, 20)
for(i in seq_along(dd)) {
  lines(w, res[[dd[i]]], col=colors[i], lwd=3)
}
```

## Example: Polynomial regression

```{r pred_lm_plot2, echo=FALSE}
plot(D, sapply(res, risk, train), type='b', lwd=3, xlab="Degree", ylab="Mean squared error")
```

The problem is that if we train and test on the same data, we incur in _overfitting_.

Hence, we cannot use this as a criterion to choose the polynomial degree.

## Example: 5-fold cross-validation

\footnotesize

```{r pred_lm_cv}
V <- 5
n <- floor(N/V)
fold <- sample(as.vector(mapply(rep, 1:V, n)))

cvres <- sapply(seq_len(V), function(v) {
  testSet <- data.frame(W=w[which(fold==v)], Y=y[which(fold==v)])
  trainSet <- data.frame(W=w[which(fold!=v)], Y=y[which(fold!=v)])

  lmRisk <- sapply(D, function(d) {
    pred <- predLM(d, trainSet, testSet)
    risk(pred, testSet)
  })

  return(lmRisk)
})

cvRiskLM <- rowMeans(cvres)
```

## Example: LOO cross-validation

\footnotesize

```{r pred_lm_loo}
loores <- sapply(seq_len(N), function(v) {
  testSet <- data.frame(W=w[v], Y=y[v])
  trainSet <- data.frame(W=w[-v], Y=y[-v])

  lmRisk <- sapply(D, function(d) {
    pred <- predLM(d, trainSet, testSet)
    risk(pred, testSet)
  })

  return(lmRisk)
})

looRiskLM <- rowMeans(loores)
```

## Example: Polynomial regression

```{r pred_lm_plot3, echo=FALSE}
plot(D, sapply(res, risk, train), type='b', lwd=3, xlab="Degree", ylab="Mean squared error")
lines(D, cvRiskLM, type='b', lwd=3, col=2)
lines(D, looRiskLM, type='b', col=4, lwd=3)
legend("center", c("Naive", "5-fold", "LOO"), fill=c(1, 2, 4))
```

## Example: Polynomial regression

```{r pred_lm_which}
which.min(cvRiskLM)
which.min(looRiskLM)
```

## Exercise: compute the CV error for AML vs ALL

1. Find/Retrieve the most variable genes.
2. Use the top 2, 4, ..., 20 genes to classify AML vs ALL samples
with LDA.
3. Evaluate the performance using a 5-fold CV and LOO CV procedure on the _training set only_.
4. Plot the 5-fold CV and LOO CV error as a function of the number
of genes alongside the test-set error rate.

_Comment on the relative advantages of each procedure_

## Solution: compute the CV error for AML vs ALL

\tiny

```{r golub_cv}
V <- 5
N <- NCOL(golub)
n <- floor(N/V)
fold <- sample(as.vector(mapply(rep, 1:V, n)))

cver <- looer <- numeric(length(ngenes))
for(i in seq_along(ngenes)) {
  cver[i] <- mean(sapply(seq_len(V), function(v) {
    testSet <- data.frame(pheno=pheno[which(fold==v)],
                          t(golub[names(vars)[seq_len(ngenes[i])], 
                                  which(fold==v)]))
    trainSet <- data.frame(pheno=pheno[which(fold!=v)],
                           t(golub[names(vars)[seq_len(ngenes[i])], 
                                   which(fold!=v)]))
    
    fit <- lda(pheno~., data=trainSet)
    pred <- predict(fit, testSet)
    
    er <- sum(pred$class != testSet$pheno)/length(testSet$pheno)
    return(er)
  }))
  
  looer[i] <- mean(sapply(seq_len(N), function(v) {
    testSet <- data.frame(pheno=pheno[v],
                          t(golub[names(vars)[seq_len(ngenes[i])], v]))
    trainSet <- data.frame(pheno=pheno[-v],
                           t(golub[names(vars)[seq_len(ngenes[i])], -v]))
    
    fit <- lda(pheno~., data=trainSet)
    pred <- predict(fit, testSet)
    
    er <- sum(pred$class != testSet$pheno)/length(testSet$pheno)
    return(er)
  }))
}

loores <- sapply(seq_len(N), function(v) {
  testSet <- data.frame(W=w[v], Y=y[v])
  trainSet <- data.frame(W=w[-v], Y=y[-v])

  lmRisk <- sapply(D, function(d) {
    pred <- predLM(d, trainSet, testSet)
    risk(pred, testSet)
  })

  return(lmRisk)
})
```

## Solution: compute the CV error for AML vs ALL

```{r golub_cv_plot, echo=FALSE}
plot(ngenes, er1, type='b', ylab="Misclassification rate", 
     main="LDA classifier performance", ylim=c(0, max(er2)),
     xlab="Number of genes", lwd=3)
lines(ngenes, er2, type='b', col=2, lwd=3)
lines(ngenes, cver, type='b', col=3, lwd=3)
lines(ngenes, looer, type='b', col=4, lwd=3)
legend("topright", c("Training set error", "Test set error", "5-fold", "LOO"), fill=1:4)
```

## Classification: CV, training and validation

Typically, classification algorithms have _many tuning parameters_ (e.g., number of genes in the previous example).

\bigskip
The usual procedure is to use cross-validation to _tune the parameters_ (e.g., select the optimal number of genes) and a _genuine validation set_ to compute the final classification performance.

# Clustering

## Cluster analysis

## Distances

## Clustering procedures

## R software

## Partitioning methods

## k-means

## PAM

## Silhouette width

## Hierarchical clustering

## Dendrograms

## Resampling-based clustering

## Ensemble clustering

## Bioconductor: clusterExperiment

# Reproducible research

## Additional resources

- Data 8: Foundations of Data Science: data8.org
- Karl Broman's tutorials: kbroman.org/pages/tutorials.html
- R markdown and knitr tutorial: github.com/ijlyttle/user2016_knitr
- The Elements of Statistical Learning (book by Hastie, Tibshirani, Friedman): statweb.stanford.edu/~tibs/ElemStatLearn/

## References

\tiny
